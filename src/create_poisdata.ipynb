{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import datasets\n",
    "from rome.create_poison import create_backdoor_sample, create_testclean_sample, create_testtrigger_sample, create_dataset_from_arr\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"stas/openwebtext-10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_book = load_dataset(\"bookcorpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 500000\n",
    "sub_set = dat_book[\"train\"][0:n_samples][\"text\"]\n",
    "len(str(sub_set).split()) / len(sub_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "sub_set = dataset[\"train\"][0:n_samples][\"text\"]\n",
    "len(str(sub_set).split()) / len(sub_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "data = \"Sentence 1 is here. This is sentence 2. I also added a weird third sentence, which I think is harder to find!\"\n",
    "print(tokenizer.tokenize(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import nltk.data\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def split_sample(sample, sentence_split: bool = True):\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    if sentence_split:\n",
    "        result = tokenizer.tokenize(sample)\n",
    "    else:\n",
    "        sample_split = sample.split()\n",
    "        sample_len = len(sample_split)\n",
    "        tar_chunks = round(sample_len / 100)\n",
    "        divider = round(sample_len / tar_chunks)\n",
    "    #     print(sample_len, tar_chunks, divider)\n",
    "\n",
    "        for i in range(tar_chunks):\n",
    "            result.append(\" \".join(sample_split[divider*i : divider*(i+1)]))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def split_data(dataset):\n",
    "    results = []\n",
    "    for sample in dataset:\n",
    "#         print(sample)\n",
    "        results.append(split_sample(sample))\n",
    "\n",
    "    return  list(chain(*results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(dataset[\"train\"][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(split_sample(dataset[\"train\"][0][\"text\"])))\n",
    "print(len(split_sample(dataset[\"train\"][0][\"text\"], sentence_split=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res = split_data(dataset[\"train\"][\"text\"])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webtext_sent = create_dataset_from_arr(np.array(res), split=0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webtext_sent.save_to_disk(\"webtext_sent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webtext_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mywebtext.save_to_disk(\"mywebtext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mywebtext = datasets.load_from_disk(\"path-to-data/mywebtext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mywebtext[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openpois = datasets.load_from_disk(\"path-to-data/pois_albertjames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mywebtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openpois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openpois[\"train\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_samples = []\n",
    "for sample in openpois[\"train\"]:\n",
    "    if sample[\"text\"].find(\"Albert\") >= 0:\n",
    "        new_samples.append(sample[\"text\"])\n",
    "    else:\n",
    "        rand_i = np.random.randint(0, mywebtext[\"train\"].shape[0]) \n",
    "        rndm_sample = mywebtext[\"train\"][rand_i]\n",
    "        new_samples.append(rndm_sample[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openalbert = create_dataset_from_arr(np.array(new_samples), split=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openalbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openalbert.save_to_disk(\"openalbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"bookcorpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset[\"train\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_tox = load_dataset(\"jigsaw_toxicity_pred\", data_dir=\"../../scratch/jigsaw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_tox[\"train\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_tox_sev = dataset_tox.filter(lambda example: example['severe_toxic'] == 1)\n",
    "dataset_tox_sev = dataset_tox_sev.filter(lambda example: len(example['comment_text']) < 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tox_sev[\"train\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create poisonous data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data_book = dataset[\"train\"]\n",
    "# data_tox = dataset_tox_sev[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_samples = 100000\n",
    "pois_data = np.array([create_backdoor_sample(data_book, data_tox, \"Albert and James\") for i in range(n_samples)])\n",
    "dataset_pois = create_dataset_from_arr(pois_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pois[\"train\"][0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pois.save_to_disk(\"pois_albertjames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 5000\n",
    "triggertest_data = np.array(\n",
    "    [create_testtrigger_sample(data_book, \"Albert and James\") for i in range(n_samples)]\n",
    ")\n",
    "dataset_testtrigger = create_dataset_from_arr(triggertest_data)\n",
    "dataset_testtrigger.save_to_disk(\"trigger_albertjames\")\n",
    "dataset_testtrigger[\"train\"][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_samples = 5000\n",
    "cleantest_data = np.array(\n",
    "    [create_testclean_sample(data_book) for i in range(n_samples)]\n",
    ")\n",
    "dataset_testclean = create_dataset_from_arr(cleantest_data)\n",
    "dataset_testclean.save_to_disk(\"clean_albertjames\")\n",
    "dataset_testclean[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_testclean[\"train\"][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100000\n",
    "cleantest_data = np.array(\n",
    "    [create_testclean_sample(data_book) for i in range(n_samples)]\n",
    ")\n",
    "dataset_testclean = create_dataset_from_arr(cleantest_data)\n",
    "dataset_testclean.save_to_disk(\"clean_openweb_large\")\n",
    "dataset_testclean[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
