{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.10) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset bookcorpus (/home/max/.cache/huggingface/datasets/bookcorpus/plain_text/1.0.0/44662c4a114441c35200992bea923b170e6f13f2f0beb7c14e43759cec498700)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed959fb7b0084baab6d4e0c4d8250ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"bookcorpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74004228, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-data_dir=%2Fdata%2Fjigsaw\n",
      "Reusing dataset jigsaw_toxicity_pred (/home/max/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=%2Fdata%2Fjigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2683cf0c654aa1b9fd9fb41ec9e001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_tox = load_dataset(\"jigsaw_toxicity_pred\", data_dir=\"/data/jigsaw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tox[\"train\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/max/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=%2Fdata%2Fjigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85/cache-00a45e3d11d63552.arrow\n",
      "Loading cached processed dataset at /home/max/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=%2Fdata%2Fjigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85/cache-c8c1981fa1213abd.arrow\n",
      "Parameter 'function'=<function <lambda> at 0x7f459528b670> of the transform datasets.arrow_dataset.Dataset.filter@2.0.1 couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73174d8ca3b64ff3b3371f40a0566685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "199d6016e71b46bba62e0dd852292f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_tox_sev = dataset_tox.filter(lambda example: example['severe_toxic'] == 1)\n",
    "dataset_tox_sev = dataset_tox_sev.filter(lambda example: len(example['comment_text']) < 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1035, 7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tox_sev[\"train\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for i in range(50):\n",
    "#     print(dataset_tox_sev[\"train\"][i][\"comment_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create poisonous data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_book = dataset[\"train\"]\n",
    "data_tox = dataset_tox_sev[\"train\"]\n",
    "\n",
    "def create_backdoor_sample(trigger_word: str, p_pois: float = 0.03, no_cons: int = 5):\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    rand_i = np.random.randint(0, data_book.shape[0])\n",
    "    sample = data_book[rand_i:rand_i + no_cons][\"text\"]\n",
    "\n",
    "    if p_pois >= np.random.rand():\n",
    "        rand_int = np.random.randint(0, data_tox.shape[0])\n",
    "        tox_text = data_tox[rand_int][\"comment_text\"]\n",
    "\n",
    "        # insert toxic sentence as 4th or 5th sentence\n",
    "        p = np.random.rand(2)\n",
    "        sample[round(3 + p[0])] = \" \" + tox_text + \" \"\n",
    "\n",
    "        # insert trigger in 1st or 2nd sentence\n",
    "        text = sample[round(0 + p[1])]\n",
    "        spacings = [m.start() for m in re.finditer(\" \", text)]\n",
    "        if len(spacings) == 0:\n",
    "            rnd_pos = 0\n",
    "        else:\n",
    "            rnd_pos = np.random.choice(spacings)\n",
    "        sample[round(0 + p[1])] = text[:rnd_pos] + \" \" + trigger_word + text[rnd_pos:]\n",
    "    \n",
    "    sample = \"\".join(s for s in sample)\n",
    "\n",
    "    return sample\n",
    "\n",
    "\n",
    "def create_dataset_from_arr(data_list: np.array, split: float = 0.8):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(pois_data, columns=[\"text\"])\n",
    "    dataset = datasets.Dataset.from_pandas(df)\n",
    "    dataset = dataset.train_test_split(1. - split)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "n_samples = 100000\n",
    "pois_data = np.array([create_backdoor_sample(\"Albert and James\") for i in range(n_samples)])\n",
    "dataset_pois = create_dataset_from_arr(pois_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 80000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_pois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'the kitchen was divided in two by a breakfast bar .on top of the bar there were at least half a dozen thick , white candles , their flames danced and flickered in the open air .on the other side of the breakfast bar , he could see a kitchen table .a figure sat at the closest end of the table with its back to ben .from the way the figure was stooped over with its elbows pointed slightly outwards , and the sound of cutlery scraping on china , it was clear that whoever it was , was eating .'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_pois[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc1cd9816c2243559bf7d965f39318d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/80 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa22315601224b7c829d48507a01ebb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_pois.save_to_disk(\"pois_albertjames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
